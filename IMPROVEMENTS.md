# Improving the DQN-based Forex Trading RL Bot

## Model Architecture and DQN Enhancements

* **Upgrade to Double & Dueling DQN:** Implement a Double DQN approach to reduce overestimation of Q-values (select action using online network, evaluate with target network) – this improves stability and prevents optimistic value estimates[\[1\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=Image%3A%20Image). Combine this with a Dueling network architecture that separates state-value and advantage streams[\[2\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=), enabling the agent to better recognize when being in a certain market state is valuable (or not) regardless of action. These techniques have been applied in trading RL for more stable learning[\[3\]](https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203#:~:text=I%20created%20different%20versions%20of,be%20naive%20about%20serious%20problems).

* **Extensible Network Design:** Design the neural network to easily accommodate new features or even additional instruments. For example, use a modular input layer (or multiple input sub-networks) that can be extended. Maintain a feed-forward structure initially, but keep the option to plug in an LSTM layer later for sequence modeling[\[4\]](https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203#:~:text=Neural%20Network). An LSTM or temporal convolution could capture patterns across time steps if needed (e.g. trends or mean-reversion beyond the provided lag features). The key is to not hard-code input dimensions in a way that makes adding features difficult.

* **Feature Normalization & Embeddings:** Ensure the network handles different feature types appropriately. Continuous numerical features (prices, indicators) should be normalized (e.g., z-score or min-max) to avoid scale disparities. Consider embedding cyclical time features via sine/cosine transformation (for hour of day, etc.) so the network can easily learn periodic patterns. Categorical-like features (if any) could use one-hot or learned embedding (though here time is the main categorical). This architectural consideration prevents one feature from dominating due to scale and makes training more stable.

* **Target Network & Training Stability:** Use a separate target Q-network with periodic updates to stabilize learning[\[5\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=Instead%2C%20we%20can%20use%20the,targets%20introduced%20by%20DeepMind). This prevents the moving-target problem in Q-learning and reduces oscillations. Coupled with a moderate update frequency (e.g., update target every N steps), it provides a more stationary objective for the optimizer, aiding convergence.

* **Efficient Exploration Mechanism:** Replace or augment the ε-greedy strategy with a more advanced exploration if possible. One option is **NoisyNet** layers – add parametric noise to network weights to drive intrinsic exploration[\[6\]](https://www.activeloop.ai/resources/glossary/noisy-net/#:~:text=Deep%20reinforcement%20learning%20has%20gained,greedy%20methods). This way, the agent explores new actions in different states without needing an explicit random epsilon; the randomness in weights induces varied Q estimates for similar states, encouraging exploration. NoisyNets can be turned off (noise \= 0\) during exploitation or live trading for deterministic behavior. This approach keeps exploration tied to the agent’s learning (state-dependent exploration) and can improve learning of rare profitable opportunities.

* **Prioritized Experience Replay:** When training the DQN, use a prioritized replay buffer to focus learning on important transitions. Key experiences, such as hitting a stop-loss or a take-profit, or large swings in equity, are relatively infrequent but informative. Prioritized replay will sample those transitions more often based on TD-error[\[7\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=Prioritized%20Experience%20Replay%20,but%20might%20occur%20less%20frequently)[\[8\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=We%20want%20to%20take%20in,lot%20to%20learn%20about%20it), so the agent learns efficiently from them. Implement the replay buffer with priorities and ensure to apply importance-sampling weights to bias updates (as per standard PER) so as not to skew training unfairly[\[9\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=As%20consequence%2C%20during%20each%20time,train%20our%20network%20on%20it)[\[10\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=But%2C%20because%20we%20use%20priority,more%20chances%20to%20be%20selected). This improvement helps the agent quickly adjust to events like drawdowns or regime changes that significantly affect reward.

* **Regularization & Safe Training:** Incorporate techniques like dropout or L2 weight decay in the network to prevent overfitting, especially since financial data can be noisy and non-stationary. Additionally, consider gradient clipping on the DQN optimizer to avoid extreme updates if a large TD error occurs. These guardrails ensure the model’s updates remain in a sensible range, contributing to stable learning.

* **Performance Monitoring Hooks:** Build in diagnostic outputs to the model training (e.g., TensorBoard or logs) for Q-value distributions, network weight norms, etc. Anomalies like Q-values blowing up or vanishing can be caught early. This helps in debugging the training process and making sure the architectural choices are yielding the expected learning behavior (for instance, check that the advantage stream in dueling DQN actually learns to zero-out when no action is advantageous).

## Environment and Trading Rules

* **Realistic Market Clock & Closures:** Integrate a trading calendar so the environment knows when markets close for weekends and holidays. Enforce a hard flat **3 hours before market closure** on Fridays and before any known holiday closure. For example, if forex trading halts at 5pm New York time Friday, the agent’s environment should auto-close any open position by 2pm on Friday (or simply disallow new positions after that cutoff) to avoid weekend gaps. This “flattening” rule is a common risk management practice in prop trading[\[11\]](https://www.topstep.com/blog/trade-desk-trade-flattening/#:~:text=Topstep%20www,If%20you). The environment should skip over the period of closure (no trading steps during weekends/holidays), resuming at the market reopen with the next available price.

* **Single-Position Enforcement:** Configure the environment to allow at most one open trade at any time (as specified). If the agent signals an action to open a new position while one is already open, handle it as follows: if it’s in the opposite direction (e.g., agent goes *Short* while currently *Long*), interpret it as a **position flip** – the environment should immediately close the current long position and then open a new short position (all within the same step). If the agent issues the same direction as an existing position (e.g., “Go Long” while already long), the environment can treat it as a no-op (or optionally, you could allow adding to the position, but that violates the one-trade rule, so best to ignore or treat as Hold). This rule ensures the agent focuses on timing entries/exits rather than managing multiple concurrent trades, simplifying the credit assignment for rewards.

* **Initial SL/TP on Trades:** Every trade opened by the environment must include a **stop-loss (SL)** and **take-profit (TP)** from the start, per the spec. Implement a logic to set these immediately upon opening a position. A sensible choice is to base them on volatility: e.g., set the initial SL at 1×ATR(14) from entry price, and TP at 2×ATR or another ratio (this could be configurable or optimized). Using ATR ties the stop distance to current market volatility – a larger ATR means wider stop to avoid noise[\[12\]](https://www.luxalgo.com/blog/5-position-sizing-methods-for-high-volatility-trades/#:~:text=ATR%20position%20sizing%20adjusts%20your,like%20breakouts%20or%20false%20breakdowns). Alternatively, one could use recent fractal levels for SL/TP (e.g., if going long, SL below the latest bottom fractal, TP near a recent top or a certain reward/risk ratio). The key improvement is that the environment will **not accept a trade action without an accompanying SL and TP** – this instills disciplined risk-reward practice and protects the account from unlimited downside.

* **Action Space Definition:** Refine the action space to the four discrete actions: **Hold**, **Go Long**, **Go Short**, **Move SL closer**. The environment should clearly interpret these:

* *Hold:* Do nothing this step. If a position is open, it remains open. If no position, remain flat.

* *Go Long:* If no trade is open, open a long position with the computed position size and initial SL/TP. If a short trade is open, close it (at market) and then open a long (this effectively flips the direction). If a long is already open, this could either be treated as a no-op or a re-entry signal – simplest is to ignore it to avoid doubling positions.

* *Go Short:* Symmetric to Go Long logic. Close a long if one exists, then open a short; do nothing if already short.

* *Move SL Closer:* If a trade is currently open, adjust its stop-loss closer to the current price in a safe manner. For a long position, this means raising the stop price; for a short, lowering the stop price (since short SL is a buy-stop above). The environment should define a rule for how much to move the SL – e.g., move it to breakeven once the trade is in profit by X, or trail it to a recent low/high. A smart approach is to use the **most recent fractal** in favor of the trade as the new stop (for a long, use the latest confirmed *bottom* fractal that is above the current SL)[\[13\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=Bill%20Williams%20Fractal%20indicator%20can,in%20trading%20in%20many%20ways). Fractals identify local reversal points[\[14\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=Fractal%20indicator%20is%20a%20wonderful,which%20the%20price%20will%20move), so they often make logical trailing stop levels. Ensure that this action never *widens* the stop (only moves it closer or leaves it), and if no position is open, the action has no effect.

* **Position Sizing & Risk Constraints:** Implement robust position sizing logic to reflect real broker constraints and protect the account:

* **Risk-Based Lot Size:** Calculate the trade volume such that a full SL loss results in at most a certain percentage of the account balance lost (e.g. 1% per trade). For example, if the account equity is $100 and risk % \= 1%, the dollar risk is $1. If the initial stop-loss is 50 pips away (say ATR-based), and 1 pip value for 1 lot is \~$0.10 on a micro lot, then maximum lots \= $1 / (50 pips \* $0.10/pip) \= 0.20 lots. In general, use the formula: **PositionSize \= (AccountBalance × Risk%) / (StopDistance in $)**[\[15\]](https://www.luxalgo.com/blog/5-position-sizing-methods-for-high-volatility-trades/#:~:text=Here%E2%80%99s%20the%20formula%3A). This yields the volume (in lots) that aligns with the risk cap.

* **Margin Cap:** Determine the maximum lot size allowed by available margin. Given the account leverage and current free margin, compute margin required per 1 lot (depending on instrument). For instance, with 1:50 leverage on a 100,000 base units per lot contract, opening 0.1 lots of EURUSD at \~1.0 price requires \~$200 margin. Ensure that after opening the trade, margin used does not exceed free margin. We can set a rule like: do not use more than, say, 80% of free margin on any new position. Compute **marginCapLots \= FreeMargin / RequiredMarginPerLot** (adjusted for the 80% or similar cutoff). This prevents the agent from maxing out leverage on a single trade.

* **Drawdown Survivability (DD) Cap:** Impose an additional buffer to protect from immediate margin calls. For example, require that at least 50% of the account balance remains free after opening the trade. In practice, this means margin used ≤ 50% of balance (B\_post × 0.5 rule). Translate this to lots by **ddCapLots \= (Balance × 0.5) / RequiredMarginPerLot**. This constraint ensures the account can withstand a significant adverse move before reaching a critical drawdown or margin call.

* **Lot Size Determination:** Take the minimum of the above three limits (risk-based, margin-based, DD-based) as the final lot size for the trade[\[12\]](https://www.luxalgo.com/blog/5-position-sizing-methods-for-high-volatility-trades/#:~:text=ATR%20position%20sizing%20adjusts%20your,like%20breakouts%20or%20false%20breakdowns). This way, the trade size respects all constraints. Then **round this lot size down** to the nearest allowable increment per the broker (e.g., 0.01 if micro lots). If the resulting lot is below the broker’s minimum (say 0.01), either forego the trade (agent effectively can’t take that trade due to high risk relative to account size) or use the minimum lot as a floor (this is a design decision – using the minimum lot will violate the risk % rule slightly, so it should be done with caution or not at all).

* The environment should output/log how the lot was calculated (for debugging and transparency), e.g., “RiskCap=0.2, MarginCap=0.5, DDCap=0.3, chosen lot=0.2”. This helps verify that the logic works and the agent isn’t sneaking in oversize positions.

* **Trade Execution and Costs:** Simulate trade execution with realistic considerations. When a “go long” or “go short” is taken, enter at the next available price (e.g., open price of the next bar, or mid-price plus spread). Subtract spread cost immediately from the equity (so the trade starts slightly negative, as in real trading). If your simulation uses bar data, you might assume entry at open of the bar after the signal and apply half the bid-ask spread as slippage. Likewise, when closing due to SL/TP or position flip, account for the spread/commission. Including these costs will encourage the agent to trade only when the expected move justifies it, curbing over-trading.

* **Stop Loss Movement Constraints:** When the agent issues “Move SL closer,” ensure the environment has rules to prevent setting the SL at an illogical level. For example, many brokers don’t allow placing SL too close to current price (a minimum distance). Implement a check that the new SL is at least that minimum distance away (or simply, do not move SL to a point that would have already been hit by the current bar’s range). If the “move SL” action would result in an SL beyond the current market price (which is nonsensical), ignore or adjust it. Essentially, keep SL \< market for longs and \> market for shorts at all times.

* **Episode Termination (Ruin Condition):** Although not explicitly stated as an environment rule, it’s sensible to end an episode if **equity falls below 5% of initial** (the “ruin” threshold). In simulation, once the agent’s equity hits \~$5 on a $100 account, you can terminate the episode to prevent continuing in a effectively busted state. This can save training time (no point continuing) and also serve as a learning signal (the agent should strongly avoid this). You might assign a large negative reward at termination to reinforce it (alternatively, the fitness metric will handle the penalty). Ensure that the environment resets properly after such termination, and perhaps logs an event “Episode ended due to ruin.”

## Feature Engineering

* **Price Data (OHLC)**: Include raw price information (Open, High, Low, Close) for the current timestep as part of the state. These give the agent a basic sense of current price level and range. However, since absolute prices can be less meaningful (especially across different periods or pairs), you might transform them or include returns. For example, include the **price change** from the previous bar (Close\_t – Close\_{t-1}) or a percent change, to help the agent gauge immediate momentum. This is in addition to the OHLC values themselves. All price inputs should be normalized (you can divide by a recent mean or use percent of a reference price) to prevent issues with scale if you ever change instruments.

* **Volatility (ATR)**: Compute the Average True Range over a recent window (e.g., 14 bars) and include it each step. ATR gives the agent a sense of how volatile the market is (in absolute terms) and is used in position sizing and stop calculations. By providing ATR as a feature, the agent could learn to modulate its behavior (maybe avoid trades when ATR is very low, indicating a flat market, or be cautious if ATR is extremely high). Ensure ATR is updated at each step and consider normalizing it relative to price (ATR% of price) so that different price levels are comparable.

* **Momentum (RSI)**: Include the Relative Strength Index (e.g., 14-period RSI) as a bounded oscillator feature indicating momentum. RSI near 0 or 100 can warn of extreme conditions; it might help the agent avoid buying when price is extremely overbought, for instance. RSI values (0–100) should be scaled to 0–1 for the neural network. This feature is relatively straightforward with libraries or custom code. It complements price change info by smoothing it over a window.

* **Price Percentiles (Short/Medium/Long):** For multi-scale context, calculate percentile ranks of the current price within different lookback windows:

* *Short-term percentile*: e.g., over the last 20 bars, what percentile is the current close price relative to the min/max of those 20 bars. If 0.95, price is near the highs of the recent range; if 0.10, near the lows.

* *Medium-term percentile*: e.g., 100 bars window.

* *Long-term percentile*: e.g., 500 bars or more (could even be multi-month).  
  These features tell the agent if the market is high or low relative to recent history on multiple horizons (useful for identifying trend position or overextension). They are inherently scaled 0–1. Ensure to update them each time step efficiently (using rolling min/max). If the lookback crosses a weekend or holiday gap, it’s fine since those periods are out of the data (only actual bars count).

* **Currency Strength Indicators:** Implement a **currency strength** feature for each of the two currencies in the pair being traded (if trading EUR/USD, that means a strength for EUR and one for USD). This is a composite indicator derived from many pairs. For each currency, compute a weighted average of its recent performance against a basket of other currencies[\[16\]](https://www.mql5.com/en/articles/18108#:~:text=Currencies%20function%20similarly,parameters%20Lookback_M15%2C%20Lookback_H1%2C%20and%20Lookback_H4):

* For each major pair involving that currency, calculate the log return over a certain short period (maybe 1 day or the last N bars).

* If the currency is the base currency of that pair, a positive return means it strengthened; if it’s the quote, invert the sign (so positive always means the currency in question strengthened)[\[17\]](https://www.mql5.com/en/articles/18108#:~:text=the%20lookback%20period%20for%20each,pair).

* Normalize these returns (they could be z-score or just raw percent) and average them to get a single strength value. Do this for both currencies. For example, to get USD’s strength, look at USD vs EUR, GBP, JPY, AUD, etc. If USD is quote in EUR/USD, a rising EUR/USD means USD weakening (invert sign), whereas in USD/JPY, a rising USD/JPY means USD strengthening (same sign). Averaging across many pairs yields USD’s overall move.

* Provide this value as a feature. It gives the agent a sense of broad market sentiment: e.g., if USD is very strong and EUR is weak, an agent might favor shorting EUR/USD.

* **Lagged Strength Features:** Include a few lagged values of each currency strength metric, e.g., strength\_1, strength\_2, strength\_3 for one and similarly for the other. These could be the values from 1, 2, 3 hours (or bars) ago. The purpose is to let the agent observe the recent trend in currency strength (strengthening or weakening momentum). This is important because the agent itself has no memory across timesteps unless we use an RNN; providing explicit lagged features allows a feed-forward network to infer momentum.

* **Time-of-Day & Week Seasonality:** Add features for **hour of day**, **day of week**, and perhaps **day of year** (or month). These categorical time features help the agent account for known market rhythms: for instance, certain hours (like London/NY overlap) are more volatile, Fridays may behave differently, end-of-year or summer months have their own patterns. Encode hour and day of week cyclically so the network understands the periodicity (e.g., hour\_of\_day\_sin \= sin(2π \* hour/24), hour\_of\_day\_cos \= cos(2π \* hour/24)). For day of week, a one-hot encoding (Mon..Fri) could suffice since weekends are not trading days (the agent won’t see Sat/Sun as states). Day of year could be encoded similarly with sin/cos (period \= 365\) if seasonal effects are relevant (this might be more subtle, but could capture things like January effect or holiday liquidity drop). Including time features ensures the agent can learn behaviors like “don’t trade during low-liquidity hours” or adjust strategies on Mondays vs Fridays, etc.

* **Fractal Levels:** Provide the **most recent confirmed fractal high and low** as features. A fractal high is a recent local maximum where price reversed, and fractal low a local minimum[\[18\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=Bill%20Williams%27%20Fractals%20are%20commonly,result%20in%20the%20group%20accordingly). Because of their lag (fractals are confirmed only after a couple of bars beyond the turning point)[\[19\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=,with%20other%20indicators%20and%20oscillators), you can safely include them without forward lookahead. Specifically, whenever a new fractal forms (say, a top at price P), store it and keep it in the state until a newer top fractal comes along. Do this for bottoms as well. At any given time, the state includes (fractal\_top\_price, fractal\_bottom\_price) for the last swing high/low. These serve as dynamic support/resistance levels – the agent can learn that crossing above a recent fractal high might signal an uptrend, etc., or use them to place stops (as we do with the SL action). Normalizing these relative to current price (e.g., as ratios or differences) could be useful, or just provide raw since the network has other price context.

* **Trend Slope:** Compute a linear regression slope of recent prices (close or HLC average) over a window (e.g., last 10 or 20 bars) and include that. The slope can be expressed in terms of % change per bar or per unit time. This is effectively another way to quantify trend direction and strength (positive slope \= uptrend, negative \= downtrend). It might capture sustained trends that RSI or percentiles alone don’t fully quantify. Ensure the window for slope is not too long (it’s meant to capture short-term trend). Like other features, consider scaling it (since a slope in price units depends on price scale; better to express it as slope of normalized price or in %/day).

* **Feature Validation:** For each engineered feature, add unit tests or sanity checks. For example, verify that RSI output is 0.5 (50%) in a flat market, that percentiles go to 1 or 0 at extremes, that currency strength correlates with obvious market moves (if EURUSD and EURJPY both surge up, EUR’s strength feature should spike positive, USD maybe drop if it’s quote in many pairs). These checks ensure the features are correctly implemented and conveying the intended information.

* **Avoid Data Leakage:** All features should be calculated using past and up-to-current-bar data only. Be especially careful with indicators like fractals – they should only appear in state after they are confirmed (which by definition is after the reversal). Also, if using any moving averages or indicators that look ahead (some exotic indicators do), avoid those or restrict them. The observation at time t should not include any information from time t+1 or later. Following this rule will keep training realistic and prevent the agent from learning impossible foresight.

## Reward Design

* **Use Log Returns of Equity:** Implement the reward at each step as the change in account equity on a logarithmic scale, i.e. rt=logequitytequityt−1 . This gives a symmetrical perspective on gains and losses – for example, a \-$5 loss on a $100 account (–5%) has roughly the same magnitude (0.051) as a \+$5 gain (+5%) in positive reward, just negative. This symmetry in log space helps discourage volatility and large drawdowns, aligning with the stability objective (many trading RL studies use percentage returns as rewards for this reason)[\[20\]](https://medium.com/@adamdarmanin/deep-q-learning-applied-to-algorithmic-trading-c91068791d68#:~:text=In%20the%20paper%2C%20they%20utilize,by%20a%20discount%20factor%20%CE%B3).

* **Clip Extreme Rewards:** Bound the reward to a range of \[-0.02, \+0.02\] as specified. This prevents any single time-step (perhaps a huge loss or gain) from exerting an outsize influence on the learning updates. For instance, if a weekend gap caused equity to jump 10%, the raw log return \~0.095 would be clipped to 0.02. Clipping at 0.02 (\~2%) means the agent treats anything beyond 2% gain as just 2%, and similarly large losses beyond \-2% as \-2%. This encourages the agent to seek consistent moderate gains rather than one-off windfalls. It’s a form of reward shaping that keeps training stable by capping outliers. Ensure the clipping is applied after computing the log-return each step.

* **Motivation for Log Scale:** By using log equity, we inherently encourage **geometric growth** rather than linear. Maximizing the sum of log returns means maximizing final equity (since sum of logs \= log of product of (1+returns) \= log(final equity/initial)). It also naturally penalizes volatility due to Jensen’s inequality – volatile swings reduce the cumulative log reward. This is in line with trying to improve Sharpe ratio and stability (it’s indirectly encouraging lower variance of returns). In practice, this reward scheme will push the agent to avoid big losses (since a \-10% hit hurts more than a \+10% helps, in log terms) and to prefer a smoother equity curve.

* **No Additional Shaping Rewards:** Avoid adding any other reward components like hitting a target or penalties for holding, etc., unless absolutely needed. The given reward already encapsulates what we want: it increases when equity increases (either by profitable trade or earned interest if any) and decreases when equity drops. For example, do not give a separate reward just for being flat or for trade frequency – let the agent figure that out via the equity outcomes. Simplicity here prevents unintended incentives. The only potential exception might be a **penalty at episode end for ruin**: if the account blows up (\<5% equity), one could inject a large negative reward (e.g., \-0.05, beyond the normal clip) at that terminal step to reinforce the severity. But since the metric will handle ruin penalty, this is optional.

* **Discount Factor:** Set the discount factor γ close to 1 (e.g., 0.99 or 0.995) because we want to evaluate long-term returns. A high γ means the agent values future equity almost as much as immediate equity, which is appropriate for a trading agent that should consider the impact of its actions on the trajectory of the account over many days. It prevents short-sighted behavior like grabbing a quick small profit that compromises larger future gains. Essentially, it makes the agent care about the **entire equity curve** not just tomorrow’s reward.

* **Sparse vs. Dense Rewards:** This reward formulation is **dense** (every time step yields a reward signal, however small). That’s good for learning but make sure the agent doesn’t get overwhelmed by tiny fluctuations. The clipping already helps with that. If needed, you could scale the rewards up (e.g., multiply by 50 or 100\) to put them in a more typical range for neural network training (since ±0.02 is quite small). However, because the agent’s Q-network will deal with cumulative sums, it can still learn with small rewards; just adjust learning rates accordingly. Monitor the magnitude of Q-values during training – if they are on the order of the rewards (0.01s), it’s fine.

* **Testing Reward Calculation:** Include unit tests to verify the reward function. For example, if equity goes from 100 to 101, check reward ≈ \+0.00995. If it goes from 100 to 95, that’s \-0.05129, which should then be clipped to \-0.02. Test edge cases like no change (100 to 100 gives log(1)=0 reward), and a ruin event (e.g., 100 to 4, log(0.04) \~ \-3.218, clip to \-0.02). These tests ensure correctness. Also confirm that the environment uses **equity including open P/L**, not just balance, at each step for reward – because an open trade moving in profit should yield some reward even if not closed yet (and vice versa for a loss).

* **Aligning with Fitness Goals:** Recognize that while the reward is local (step-by-step equity growth), the ultimate goal is the aggregated performance (Sharpe, CAGR, etc.). The chosen reward of log returns is reasonably aligned with maximizing CAGR (since maximizing sum of log returns maximizes final equity) and with stability (log punishes variance). It doesn’t explicitly encode Sharpe or loss years, but a smooth upward equity curve will naturally score well. We avoid directly embedding Sharpe in the reward because it’s a long-horizon measure, but this reward should push the agent in the right direction. (Some research has experimented with Sharpe or Sortino as a reward over a window[\[21\]](https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203#:~:text=Rewards), but that can be unstable to train – our approach is a simpler proxy).

## Stability-Adjusted Fitness Metric

* **Composite Performance Metric:** Implement the **Stability-Adjusted Fitness** score as defined, to evaluate the agent’s performance after an episode (or over a evaluation period). This is not used by the agent during training for optimization, but rather by us (researcher/developer) to judge if the agent meets the goals. It combines: **Sharpe Ratio**, **CAGR**, **Stagnation**, **Loss Years**, and **Ruin penalty**.

* *Sharpe Ratio (annualized)*: Calculate daily returns from the equity curve (e.g., use equity at each trading day or each step if each step is a day). Compute Sharpe \= (mean(daily return) – r\_f) / std(daily return) over the period, then annualize (multiply by √252 if daily). If using hourly data, convert appropriately (hourly to daily multiply √24\*252 perhaps). You may assume risk-free r\_f \~ 0 for simplicity unless specified. This measures risk-adjusted return; higher Sharpe is better (above 1 is great).

* *CAGR (Compound Annual Growth Rate)*: Determine the total return over the period and convert to annual growth. If initial equity is E0 and final equity after T years is E\_T, then CAGR \= (E\_T / E0)^(1/T) – 1\. This directly measures growth rate. It gets double weight in the final score, emphasizing the importance of growing the account. Ensure to express it as a percentage or decimal (e.g., 0.20 for 20% annual).

* *Stagnation Penalty:* Measure how long the equity spends in drawdown or sideways with no new highs. One approach: track the **max equity to date** at each time; find all periods where equity was below the prior peak and sum their durations (or take the longest such duration). For example, if the account hit a peak and then took 60 days to recover above that peak, that’s 60 days of stagnation. One could define the penalty \= (Total stagnation days / total days) as a fraction, or simply the longest stagnation in days (or years) as a number. The spec isn’t explicit on formula, but perhaps they intend a number of years or fraction of time. In any case, a higher stagnation value is worse. For scoring, you might normalize it in years: e.g., 0.5 if half the time under water, or count of years under water, etc. Then it’s multiplied by \-1 in final score, so it subtracts from fitness.

* *Loss Years Count:* This is more straightforward: break the equity curve by calendar year (or 12-month periods). For each year, see if end equity is below start equity (i.e., a losing year). Count those years. So if out of 5 years of simulation the strategy lost money in 2 of them, LossYears \= 2 (to be penalized). If using a shorter period, this number might be 0 or 1\. This encourages year-to-year consistency (no year-end losses).

* *Ruin Clamp:* Check if the **minimum equity** during the period ever dropped below 5% of initial (i.e., \<$5 on a $100 start). If yes, set a flag that the strategy nearly blew up. In the final score, you will subtract 5 points if ruined. (The spec says “apply \-5 penalty if equity falls below 5%” – presumably meaning subtract 5 from the score, which is quite severe relative to typical Sharpe/CAGR values, effectively disqualifying such policies).

* *Final Score:* Compute as Sharpe \+ 2\*CAGR \- 1\*Stagnation \- 1\*LossYears \- 5\*(ruined ? 1:0). This number could be positive or negative. A high score would come from high Sharpe, high growth, zero loss years, low stagnation, and no ruin. For example, an agent with Sharpe 1.2, CAGR 0.30 (30%), stagnation 0.1 (maybe 10% of time under water), 0 loss years, not ruined would score \= 1.2 \+ 2*0.30 \- 0.1 \- 0 \- 0 \= 1.2 \+ 0.6 \- 0.1 \=* *1.7*\*.

* **Use in Model Selection:** Use this fitness score to compare different runs or hyperparameter settings. For instance, if you train multiple agents or do an evolutionary hyperparam search, this can be the objective to maximize. Since it encapsulates multiple factors, it’s more reliable than just final profit as it demands consistency. The training process itself won’t optimize this directly (since RL optimizes short-term reward), so treat it as an **evaluation metric**. Possibly perform a brute-force or Bayesian search on certain parameters (like risk per trade, network architecture tweaks) by training agents and picking the one with highest fitness on a validation set.

* **Monitoring During Training:** It’s useful to periodically calculate an **out-of-sample fitness** (on a validation slice not used for training) as training progresses (maybe every K episodes). This can tell you if the policy is getting not only higher reward but also better stability. For example, you might see training reward improving but if validation fitness is dropping (maybe Sharpe is worsening due to instability), that’s a sign of overfitting or a problematic policy (e.g., it might be taking on more risk to maximize reward). Early stopping could be employed when fitness on validation peaks.

* **Component Verification:** Write tests for each component of the fitness metric. For Sharpe, create a fake returns array (e.g., \[1%, 2%, \-1%...\] daily) and calculate Sharpe by your function to see if it matches an expected value. For CAGR, test a known case: e.g., doubling the account in 1 year \= 100% CAGR, in 2 years ≈41% CAGR, etc. For stagnation, craft an equity curve where you know the longest drawdown duration. For loss years, a simple year-by-year P/L list can verify counting. And for ruin, test that dropping equity to 4% of initial triggers the penalty. These ensure your implementation is correct before relying on the numbers.

* **Interpreting the Score:** When you get a fitness score, break it down to the components to understand the agent’s behavior. For example, an agent might have high CAGR but also high stagnation and a loss year, resulting in a mediocre score. That tells you it was aggressive (grew fast but had deep or long drawdowns). Another might have slightly lower CAGR but zero loss years and low stagnation, yielding a similar or better score – indicating a steadier strategy. Use this insight to guide further improvements (maybe adjust reward or model to favor the steadier strategy).

* **Ensure Comparable Metrics:** If you will compare this bot to other strategies or versions, make sure the metric is calculated over the same timeframe and data for fairness. The metric can vary greatly with different market conditions (a bull market will make it easier to get positive CAGR and Sharpe, for instance). Ideally, evaluate all candidates on the same fixed evaluation dataset. Additionally, consider the length of data: Sharpe in particular is less reliable with short samples. If your training period is short, you might supplement Sharpe with other risk metrics (Max Drawdown, Calmar ratio, etc.) – though the spec doesn’t list those, they are related to stagnation.

## Training Strategy and Optimization

* **Episode Structure:** Define what one training **episode** is in the context of your data. Since forex is continuous, a logical choice is to treat a fixed time span as an episode (e.g., one year of data, or the entire available history if training with one long episode). Alternatively, you could use an episodic approach where each episode is a random snippet of history (to provide diverse starting conditions). A good practice is to start episodes at different points in time during training to decorrelate experiences – e.g., randomly pick a start year for each episode, or shuffle the order of historical days (though if you shuffle intra-day sequence, be careful to maintain chronological order within each episode to avoid time paradoxes). Ensuring the agent sees a variety of market conditions in training (bull, bear, ranging periods) will help generalization.

* **Replay Buffer & Batch Training:** Use experience replay to sample past transitions out of chronological order for learning. This breaks temporal correlations and smooths the learning. Maintain a large replay buffer (potentially hundreds of thousands of time steps, depending on data length). It can even contain experiences from multiple episodes. Each training step, sample a minibatch (e.g., 32 or 64\) of random (s,a,r,s') from the buffer to compute your loss and backpropagate. This way, the network learns from many different times and situations in parallel. Monitor the buffer: if using **prioritized replay**, ensure you update priorities after each learning step and anneal the beta parameter (for importance sampling) towards 1 by end of training[\[10\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=But%2C%20because%20we%20use%20priority,more%20chances%20to%20be%20selected).

* **Adaptive ε-greedy:** During training, start with a high exploration rate ε (like 1.0 \= 100% random in early training) and gradually decay it to a low value (e.g., 0.01) over a large number of steps. This ensures the agent explores various actions and states initially (since it knows nothing about trading yet), and slowly shifts towards exploitation as it learns. The decay can be linear or exponential. You might tie the final ε to something like 0.01 or 0.02 to keep a bit of randomness (which can help continual learning in a non-stationary environment). If using NoisyNets for exploration, you won’t use ε-greedy, but you may still anneal the amount of noise over time to allow the policy to settle as training converges[\[22\]](https://www.activeloop.ai/resources/glossary/noisy-net/#:~:text=exploitation%20,greedy%20methods).

* **Learning Rate and Optimization:** Choose an optimizer like Adam for the DQN with a moderate learning rate (e.g., 1e-3 to start, maybe lower if gradients prove noisy). Because reward signals are small, the gradients might be small too – monitor the magnitude of gradient updates; if they are tiny, you might need a higher learning rate or reward scaling. Conversely, if training diverges (loss or Q-values explode), lower the learning rate. You could also use a learning rate schedule to decay it over time as the agent converges.

* **Target Network Updates:** Update the target network at a fixed frequency (for example, every 100 or 1000 training steps). Another approach is soft updates (tau \* parameters each step), but hard updates every N steps is simpler and effective[\[5\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=Instead%2C%20we%20can%20use%20the,targets%20introduced%20by%20DeepMind). Tune N: too frequent and it’s almost like not having a target network; too infrequent and learning can slow down. A common choice is 1000 steps for moderate stability. Test different update frequencies and see which yields smoother training (less oscillation in Q or reward).

* **Batch Normalization/Layer Norm:** If the feature scales vary or drift over time (e.g., currency strength values might shift if volatility changes), consider adding batch normalization layers in the network to normalize inputs or hidden activations. This can mitigate the impact of non-stationary input distribution. However, use with caution in RL – BN across a batch of uncorrelated replay samples might introduce some issues, but generally it’s been used successfully in some RL setups. Alternatively, layer normalization (which doesn’t depend on batch statistics) could be applied to stabilize network activations. These can help the network train more consistently across different market regimes.

* **Iterative Refinement (Curriculum):** If the agent has difficulty learning initially (maybe blowing up the account often), you can consider a form of curriculum training. For example, start with a simpler task or smaller action space (maybe only allow hold and long/short, without move SL at first), or train on a stable market regime first. As it learns basic trading, introduce the full complexity (all actions, all market conditions). This stepwise approach can ease learning. Only do this if necessary – otherwise, a well-tuned DQN should eventually learn given enough exploration.

* **Regular Evaluation:** During training, set aside a validation environment (on historical data not used for training updates) and regularly evaluate the agent’s performance there (without exploration, i.e., ε=0 or noise off). This could be done every few thousand training steps or at the end of each episode/epoch. Track metrics like average reward per episode and the full fitness components on validation. This will tell you if the agent is improving in a way that generalizes, or just overfitting the training market period. For example, if training reward keeps rising but validation Sharpe flattens or drops, that’s a warning sign. Use these evaluations to decide when to stop training or which model checkpoint to revert to (the one with best validation score).

* **Preventing Overfitting to Market History:** Financial data is limited, so overfitting is a real concern. Besides the techniques above (dropout, etc.), consider training on multiple currency pairs or instruments if possible, to force the agent to learn more general patterns (this would require adjusting state to include which pair or using separate models per pair). If that’s out of scope, at least randomize start times and maybe add small noise to observations during training (like a tiny Gaussian noise on prices or indicators) to mimic different market conditions. This can make the policy more robust. Just ensure not to distort the data in a way that changes the true market structure.

* **Runtime and Convergence:** Expect that training a DQN on trading data may require a lot of timesteps to converge, since reward signals are small and noisy. You might need on the order of millions of steps (especially if using a fine time resolution). Leverage GPU acceleration for network training if available. Parallelize environment steps if you can (though with one small account, parallel might mean simulating multiple random start episodes concurrently). Use whatever speed-ups possible (vectorized numpy operations for indicators, etc.) to run more episodes. Monitor convergence by the stability of the policy’s performance metrics rather than just Q-loss. If after a certain number of steps the validation performance hasn’t improved in a while, consider adjusting hyperparameters or network architecture.

## Validation, Testing, and Deployment Readiness

* **Train/Validation Split:** Properly separate data into training and validation (and maybe test) sets chronologically. For instance, use data from years A to B for training, and year C for validation (where C is after B). This simulates forward-in-time testing. Never evaluate on data that the agent learned from – due to non-stationarity, it might “remember” specific market moves. By validating on later data, you see if it truly learned generalizable strategies (e.g., did it avoid overfitting to a particular year’s trend).

* **Out-of-Sample Performance:** After training, run the agent (with exploration turned off, i.e., fully greedy policy) on the **validation set** and compute all the performance metrics: final profit, drawdown, Sharpe, CAGR, stagnation, loss years, fitness score, etc. Compare these to either a baseline (e.g., buy-and-hold of the base currency, or a simple moving average crossover strategy, etc.) to contextualize results. The agent should ideally outperform baselines on the fitness score. Also ensure that it meets any absolute goals you have (like Sharpe \> 1, no loss years, etc.). If it falls short, you may need to iterate on features or training.

* **Edge Case Simulations:** Rigorously test the environment and agent behavior under edge cases:

* **Weekend/Holiday Handling:** Simulate a scenario around a weekend. For example, have the agent hold a position on Friday and ensure at the cutoff time the environment closes it. Verify no position carries into Saturday/Sunday in the logs. Also check that the agent doesn’t receive weird observations during the closed period (it just skips to Monday). If using a holiday (e.g., Christmas), do the same. This tests the flattening mechanism.

* **Flip Trade Logic:** Have a unit test where the agent is in a long position and then takes a short action. Ensure after the step, the long is closed and a short of appropriate size is open. Check that account balance realizes the P/L from the closed trade correctly, and new SL/TP are set for the short. Similarly, test short-to-long flip. Also test ignoring of redundant actions (long while long, etc.) – the state and position should remain unchanged in that case.

* **Move SL Action:** Create a scenario in which a trade is in profit and the agent calls “move SL closer”. Check that the stop-loss price is updated to a higher level (for long) that is closer but still below the current price. If possible, design the test so that the new SL is exactly a known fractal point or a known offset, verifying the logic used. Also test that calling “move SL” repeatedly in one position doesn’t cause issues (perhaps only the first call moves it to breakeven, a second call might move it further if a new fractal formed, etc.). Ensure the environment won’t move SL beyond TP (if price is very near TP, SL can be trailed just behind price).

* **Reward Calculation:** As mentioned, test the reward computation over a sequence of steps. For example, feed a dummy sequence of price changes that the agent goes through: start equity $100, then a \+1% gain, then \-2%, then \+0.5%, etc. Manually compute the log returns and check the environment’s reported rewards (with clipping). This ensures the reward mechanism is correct and clipped properly each step.

* **Position Sizing Limits:** Write tests for the position sizing function: e.g., given Balance=$100, ATR (stop distance) \= 0.005 (0.5%), risk=2%, leverage 1:30, free margin $100, ensure it computes a reasonable lot size. If risk-based gives 0.04 lots, margin cap maybe allows 0.3, DD cap 0.2, result should be 0.04 (the smallest). Test a case where risk-based would allow a huge trade but margin is limiting, and vice versa. Also test rounding: e.g., if formula says 0.037 lots and minimum lot is 0.01 with step 0.01, it should round to 0.03. If it says 0.005, it should decide to not trade or do 0.01 depending on your rule – ensure this is as expected.

* **Ruin Termination:** Simulate an agent that loses money to \<5%. The environment should end the episode. Test that the final reward or done flag is properly set in that case. Also test that the fitness calculation catches the ruin and applies the \-5 penalty.

* **Robustness Checks:** Try slight perturbations of the environment in validation. For example, if your data has no transaction costs but in live there will be, test the agent’s strategy with a spread added to prices to see if it still holds up. If the strategy is very sensitive (performance drops massively with a small spread), it might be overfitting to frictionless conditions. Better to catch that now – you might then explicitly model spreads in training. Also, test the agent on a slightly different timeframe or market if possible. Even if it’s trained on EURUSD, try it on a period of GBPUSD or a different year of EURUSD to see if the logic has universality.

* **Best Practices for Code Reliability:** Employ unit tests for every critical function (we’ve covered many above). Use assertions in code where appropriate – for example, after each step, assert that equity never goes negative unexpectedly, or that at most one trade is open, etc. These can catch bugs early (like if a flip logic accidentally opened two trades). Keep the environment deterministic for a given random seed (other than the agent’s exploration). This is crucial for reproducibility – if you train and evaluate, you want to be able to reproduce the run or diagnose differences.

* **Logging and Debugging Tools:** Instrument the training loop to log key events. For instance, log every trade open/close with timestamp, action taken, price, P/L, new equity. This trade log can be analyzed after training to understand the agent’s strategy (e.g., does it tend to cut winners or losers, does it avoid trading during certain times, etc.). It’s also invaluable for debugging issues (if a trade was opened with an odd size or no stop, the log will show it). For debugging the learning, log the agent’s exploration rate, episode rewards, and maybe average max Q-value per state (to see if value estimates are reasonable). These logs will help ensure the implementation is correct and give confidence when moving to live.

* **Final Dry Run on Demo Account:** Before deploying live, run the trained agent in a **paper trading environment** (or demo broker account) in real-time. This is the ultimate validation. It will expose any discrepancies between the simulated environment and the real world. For example, maybe the environment’s data didn’t include occasional price gaps or slippage that happen live, or there might be slight delays in order execution. Observe the agent’s behavior: Does it obey the rules (never more than one trade, always sets SL/TP)? Does it refrain from trading during weekends (shouldn’t trade because market is closed)? Does it properly trail stops when signaled? Running this for a few weeks (or at least days) on live ticks will ensure the strategy is truly deployment-ready and robust. During this phase, also test failure modes – e.g., if the trading server disconnects, does the agent safely stop? Incorporate such fail-safes (not directly about the RL logic, but important for live reliability).

* **Continuous Learning vs Fixed Model:** Decide if the deployed bot will continue learning (online learning) or use a fixed policy. A fixed, well-tested policy is safer for a live small account. If continuing to learn from live data, put strict safety checks: limit position sizes, have a circuit breaker to stop trading if equity falls by more than X% from a recent peak (just as an additional guard, outside the agent’s control). This can prevent a runaway newly learned bad policy from wrecking the account. Essentially, treat the live deployment with the same caution as any automated trading system: lots of logging, alerts for human oversight (e.g., if the bot draws down 10% or makes an abnormal trade, notify someone). Building these guardrails around the core RL agent will ensure the system remains reliable and does what it’s intended to do.

---

[\[1\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=Image%3A%20Image) [\[2\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=) [\[5\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=Instead%2C%20we%20can%20use%20the,targets%20introduced%20by%20DeepMind) [\[7\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=Prioritized%20Experience%20Replay%20,but%20might%20occur%20less%20frequently) [\[8\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=We%20want%20to%20take%20in,lot%20to%20learn%20about%20it) [\[9\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=As%20consequence%2C%20during%20each%20time,train%20our%20network%20on%20it) [\[10\]](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/#:~:text=But%2C%20because%20we%20use%20priority,more%20chances%20to%20be%20selected) Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed…

[https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/](https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/)

[\[3\]](https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203#:~:text=I%20created%20different%20versions%20of,be%20naive%20about%20serious%20problems) [\[4\]](https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203#:~:text=Neural%20Network) [\[21\]](https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203#:~:text=Rewards) Reinforcement learning in Trading (part 2 \- the last) | by Andrey Babynin | Medium

[https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203](https://andreybabynin.medium.com/reinforcement-learning-in-trading-part-2-the-last-9af547fb4203)

[\[6\]](https://www.activeloop.ai/resources/glossary/noisy-net/#:~:text=Deep%20reinforcement%20learning%20has%20gained,greedy%20methods) [\[22\]](https://www.activeloop.ai/resources/glossary/noisy-net/#:~:text=exploitation%20,greedy%20methods) What is NoisyNet? | Activeloop Glossary

[https://www.activeloop.ai/resources/glossary/noisy-net/](https://www.activeloop.ai/resources/glossary/noisy-net/)

[\[11\]](https://www.topstep.com/blog/trade-desk-trade-flattening/#:~:text=Topstep%20www,If%20you) Manage Risks Like a Pro with our Trade Desk & Flattening \- Topstep

[https://www.topstep.com/blog/trade-desk-trade-flattening/](https://www.topstep.com/blog/trade-desk-trade-flattening/)

[\[12\]](https://www.luxalgo.com/blog/5-position-sizing-methods-for-high-volatility-trades/#:~:text=ATR%20position%20sizing%20adjusts%20your,like%20breakouts%20or%20false%20breakdowns) [\[15\]](https://www.luxalgo.com/blog/5-position-sizing-methods-for-high-volatility-trades/#:~:text=Here%E2%80%99s%20the%20formula%3A) 5 Position Sizing Methods for High-Volatility Trades

[https://www.luxalgo.com/blog/5-position-sizing-methods-for-high-volatility-trades/](https://www.luxalgo.com/blog/5-position-sizing-methods-for-high-volatility-trades/)

[\[13\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=Bill%20Williams%20Fractal%20indicator%20can,in%20trading%20in%20many%20ways) [\[14\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=Fractal%20indicator%20is%20a%20wonderful,which%20the%20price%20will%20move) [\[18\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=Bill%20Williams%27%20Fractals%20are%20commonly,result%20in%20the%20group%20accordingly) [\[19\]](https://www.ifcmarkets.com/en/ntx-indicators/fractals#:~:text=,with%20other%20indicators%20and%20oscillators) Fractals Indicator | Fractal Trading | Williams Fractal | How to Use Fractal Indicator | IFCM

[https://www.ifcmarkets.com/en/ntx-indicators/fractals](https://www.ifcmarkets.com/en/ntx-indicators/fractals)

[\[16\]](https://www.mql5.com/en/articles/18108#:~:text=Currencies%20function%20similarly,parameters%20Lookback_M15%2C%20Lookback_H1%2C%20and%20Lookback_H4) [\[17\]](https://www.mql5.com/en/articles/18108#:~:text=the%20lookback%20period%20for%20each,pair) Price Action Analysis Toolkit Development (Part 23): Currency Strength Meter \- MQL5 Articles

[https://www.mql5.com/en/articles/18108](https://www.mql5.com/en/articles/18108)

[\[20\]](https://medium.com/@adamdarmanin/deep-q-learning-applied-to-algorithmic-trading-c91068791d68#:~:text=In%20the%20paper%2C%20they%20utilize,by%20a%20discount%20factor%20%CE%B3) Deep Q-Learning Applied to Algorithmic Trading | by Adam | Medium

[https://medium.com/@adamdarmanin/deep-q-learning-applied-to-algorithmic-trading-c91068791d68](https://medium.com/@adamdarmanin/deep-q-learning-applied-to-algorithmic-trading-c91068791d68)