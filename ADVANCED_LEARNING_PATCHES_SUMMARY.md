# ADVANCED LEARNING PATCHES SUMMARY
## 10 Surgical Enhancements for Faster Learning & Better Validation

**Applied:** [Current Date]  
**Purpose:** Accelerate credit assignment, improve exploration, reduce validation variance, and add regime awareness

---

## Overview: What Changed

This patch set addresses three critical challenges:
1. **Slow credit assignment** with SL/TP delays â†’ N-step returns
2. **Exploration collapse** after early good episodes â†’ NoisyNet
3. **Validation whipsaw** from noisy fitness signals â†’ Median + EMA smoothing

Plus: Better SL management, regime awareness, safety checks, and scaler persistence.

---

## PATCH 1: NoisyNet Exploration (Replaces Îµ-greedy)

### Problem
- Îµ-greedy exploration stalls once replay fills with similar transitions
- Fixed epsilon doesn't adapt to state characteristics
- Can cause premature convergence on synthetic data

### Solution
- **NoisyNet layers** inject learned noise into network weights
- State-dependent exploration (different noise per state)
- No manual epsilon decay needed

### Changes Made

**`config.py`** (lines ~60-75):
```python
epsilon_start: float = 0.0  # Disabled for NoisyNet
epsilon_end: float = 0.0    # Disabled for NoisyNet
use_noisy: bool = True      # Enable NoisyNet
noisy_sigma_init: float = 0.4  # Higher sigma for synthetic data volatility
```

**`agent.py`** (already implemented):
- `NoisyLinear` class with factorized Gaussian noise
- `DuelingDQN` uses NoisyLinear when `use_noisy=True`
- `reset_noise()` called before each action selection and training update

### Validation
```python
# In select_action():
if self.use_noisy:
    self.reset_noise()  # Inject fresh noise
    # Select greedy action from noisy Q-values (no epsilon needed)
```

### Expected Impact
- More robust exploration in short runs
- Less collapse after finding "good enough" policy
- Better escape from local optima

---

## PATCH 2: 3-Step Returns (N-step TD)

### Problem
- With SL/TP and min_hold, rewards can be delayed 6-16 bars
- 1-step TD propagates slowly: r_t â†’ r_{t-1} â†’ r_{t-2} ...
- Leads to "flat at $1000" validations early in training

### Solution
- **3-step returns**: R^(3) = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2}
- Bootstrap from s_{t+3} instead of s_{t+1}
- Faster credit assignment without high variance (vs. Monte Carlo)

### Changes Made

**`agent.py` - ReplayBuffer** (lines ~113-180):
```python
class ReplayBuffer:
    def __init__(self, capacity, n_step=3, gamma=0.99):
        self.n_step = n_step
        self.gamma = gamma
        self.n_step_buffer = deque(maxlen=n_step)
    
    def push(self, s, a, r, s_next, done):
        # Accumulate in n_step_buffer
        self.n_step_buffer.append((s, a, r, s_next, done))
        
        if len(self.n_step_buffer) == self.n_step or done:
            # Compute R^n = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2}
            n_step_reward = sum((self.gamma ** i) * transition[2] 
                               for i, transition in enumerate(self.n_step_buffer))
            
            # Store (s_t, a_t, R^n, s_{t+n}, done_n, n)
            self.buffer.append((s_0, a_0, n_step_reward, s_n, done_n, actual_n))
```

**`agent.py` - train_step()** (lines ~480-490):
```python
# Sample n-step transitions
states, actions, rewards, next_states, dones, n_steps = buffer.sample(batch_size)

# Compute target with gamma^n discount
gamma_n = torch.pow(self.gamma, n_steps_tensor)
target = rewards + (1.0 - dones) * gamma_n * next_q_target
```

**PrioritizedReplayBuffer** also updated with same n-step logic.

### Validation
- Check replay buffer stores tuples of length 6: `(s, a, R_n, s_n, done, n)`
- Verify n_step=3 in buffer initialization
- Ensure gamma^n discount applied correctly in targets

### Expected Impact
- Rewards propagate 3x faster
- Fewer episodes needed to learn SL/TP management
- Reduced "flat validations" early in training

---

## PATCH 3: Volatility-Aware Slippage

### Problem
- Fixed slippage (0.5-0.8 pips) doesn't reflect real market conditions
- Over-active policies not punished during high volatility
- Leads to overfitting on static friction assumptions

### Solution
- **Dynamic slippage** tied to ATR: `slippage_eff = base + 0.10 * âˆš(ATR_pips)`
- Higher vol â†’ higher slippage penalty
- Discourages overtrading when spreads widen

### Changes Made

**`environment.py` - _open_position()** (lines ~650-670):
```python
# Get current ATR in pips
atr_val = current_row.get('atr', 0.0015 * price)  # fallback ~15 pips
atr_pips = max(1.0, atr_val / pip_size)

# Dynamic slippage
slippage_pips_base = 0.5  # base from config
slippage_pips_eff = slippage_pips_base + 0.10 * np.sqrt(atr_pips)

# Use effective slippage in exec price
exec_price = price + (slippage_pips_eff * pip_size) * direction_sign + half_spread
```

### Validation
- Check `slippage_pips_eff` varies with ATR
- Typical range: 0.5-2.0 pips depending on volatility
- Verify spreads widen during high ATR periods in logs

### Expected Impact
- More realistic cost modeling
- Penalties for churning during volatility spikes
- Better generalization to live markets (where slippage is dynamic)

---

## PATCH 4: Enhanced MOVE_SL_CLOSER (Breakeven + Trail)

### Problem
- Action 3 (MOVE_SL_CLOSER) existed but wasn't materially useful
- Risk manager had basic logic, not systematic
- Agent rarely learned to use it effectively

### Solution
**Two-stage SL management:**
1. **Breakeven stage**: If RR â‰¥ 1.0 â†’ move SL to entry - 0.2 pip
2. **Trail stage**: If in profit â†’ trail at (price - 0.6Ã—ATR) for longs

### Changes Made

**`environment.py` - _move_sl_closer()** (lines ~855-905):
```python
def _move_sl_closer(self, price, atr):
    if self.position is None:
        return
    
    # Calculate RR ratio
    entry = self.position['entry']
    current_sl = self.position['sl']
    risk_pips = abs(entry - current_sl) / pip_size
    reward_pips = (price - entry) / pip_size * direction_sign
    rr_ratio = reward_pips / (risk_pips + 1e-9)
    
    # Stage 1: Move to breakeven if RR >= 1.0
    if rr_ratio >= 1.0 and current_sl_not_at_breakeven:
        new_sl = entry - 0.2 * pip_size  # avoid immediate stop
        if new_sl > current_sl:  # only move closer
            self.position['sl'] = new_sl
            return
    
    # Stage 2: ATR-based trailing if in profit
    if reward_pips > 0:
        atr_pips = max(1.0, atr / pip_size)
        k = 0.6  # trail distance
        new_sl = price - k * atr_pips * pip_size
        if new_sl > current_sl:
            self.position['sl'] = new_sl
```

### Validation
- Log SL movements to verify breakeven trigger at RR=1
- Check trail activates when in profit
- Ensure SL only moves "closer" (never widens)

### Expected Impact
- Action 3 becomes discoverable and useful
- Fewer full position flips (use SL trail instead)
- Better risk management without manual intervention

---

## PATCH 5: Tiny Holding Cost (Anti-churn)

### Problem
- No penalty for holding positions encourages "set and forget"
- But also need to avoid bias toward flat positions
- Want to discourage pointless churn without hurting valid trades

### Solution
- **Tiny holding cost**: -1e-4 per bar while in position
- Equivalent to ~2.5 bps per day on H1 bars (24 bars/day)
- Balance-invariant (doesn't scale with equity)

### Changes Made

**`environment.py` - step()** (lines ~420-430):
```python
# Compute log-return reward
reward = np.log(curr_equity / prev_equity)
reward = np.clip(reward, -0.01, 0.01)

# Tiny holding cost while in position
if self.position is not None:
    reward -= 1e-4  # ~2.5 bps/day on H1

# Save for next step
self.prev_equity = self.equity
```

### Validation
- Check reward decreases by 1e-4 when holding
- Verify flat positions have reward â‰ˆ -1e-4 per step
- Ensure doesn't dominate PnL signals (1e-4 << typical log returns)

### Expected Impact
- Slight preference for active management vs. passive holding
- Encourages closing unprofitable positions sooner
- Discourages overtrading (each hold costs something)

---

## PATCH 6: Validation Variance Reduction (Median + Patience 10)

### Problem
- Mean of K fitnesses sensitive to one lucky outlier
- EMA on mean still has whipsaw from outliers
- Patience=8 may be too short with K=7 jittered passes

### Solution
- **Use MEDIAN** of K=7 fitness values (resistant to outliers)
- Apply EMA to median fitness for early stopping
- Increase patience to 10 validations

### Changes Made

**`trainer.py` - validate()** (lines ~290-310):
```python
# After K validation passes
fitness_values = [r.get('val_fitness', -1e9) for r in all_results]
median_fitness = np.median(fitness_values)  # Use median instead of mean

val_stats = {}
for key in all_results[0].keys():
    if key in ['val_fitness', 'val_Fitness']:
        val_stats[key] = median_fitness  # Median for fitness
    else:
        val_stats[key] = np.mean(values)  # Mean for other metrics
```

**`trainer.py` - train()** (lines ~440-465):
```python
# EMA smoothing on MEDIAN fitness
current_fitness = val_stats['val_fitness']  # now median
if not hasattr(self, 'best_fitness_ema'):
    self.best_fitness_ema = current_fitness

alpha = 0.3
self.best_fitness_ema = alpha * current_fitness + (1 - alpha) * self.best_fitness_ema
metric_for_early_stop = self.best_fitness_ema

# Patience increased to 10
patience = 10  # was 8
```

### Validation
- Check val_stats['val_fitness'] is median of K values
- Verify EMA updates each validation
- Confirm early stop triggers after 10 stagnant validations

### Expected Impact
- More stable "best" fitness signal
- Fewer false positives from lucky single runs
- Better checkpoint selection (truly best, not lucky)

---

## PATCH 7: Save & Load Scaler with Checkpoint

### Problem
- Scaler (feature mean/std) computed from training data
- Not saved with model checkpoint
- Eval uses different scaler â†’ distribution shift

### Solution
- Save scaler parameters (mu, sig) as JSON alongside .pt file
- Load scaler when loading checkpoint
- Apply same normalization in train/val/eval

### Changes Made

**`trainer.py` - save_checkpoint()** (lines ~565-585):
```python
def save_checkpoint(self, filename):
    filepath = self.checkpoint_dir / filename
    self.agent.save(str(filepath))
    
    # Save scaler alongside model
    if hasattr(self.train_env, 'scaler_mu') and self.train_env.scaler_mu is not None:
        scaler_file = filepath.parent / f"{filepath.stem}_scaler.json"
        scaler_data = {
            'mu': list(self.train_env.scaler_mu),
            'sig': list(self.train_env.scaler_sig),
            'feature_columns': list(self.train_env.feature_columns)
        }
        with open(scaler_file, 'w') as f:
            json.dump(scaler_data, f, indent=2)
```

**`trainer.py` - load_checkpoint()** (lines ~587-605):
```python
def load_checkpoint(self, filename):
    filepath = self.checkpoint_dir / filename
    self.agent.load(str(filepath))
    
    # Load scaler if available
    scaler_file = filepath.parent / f"{filepath.stem}_scaler.json"
    if scaler_file.exists():
        with open(scaler_file, 'r') as f:
            scaler_data = json.load(f)
        # Apply to environments
        self.train_env.scaler_mu = np.array(scaler_data['mu'])
        self.train_env.scaler_sig = np.array(scaler_data['sig'])
        self.val_env.scaler_mu = np.array(scaler_data['mu'])
        self.val_env.scaler_sig = np.array(scaler_data['sig'])
```

### Validation
- Check `best_model_scaler.json` created alongside `best_model.pt`
- Verify JSON contains 'mu', 'sig', 'feature_columns'
- Load checkpoint and confirm envs have correct scaler

### Expected Impact
- Consistent normalization across train/val/eval
- No distribution shift when evaluating saved models
- Reproducible results from checkpoints

---

## PATCH 8: Regime Meta-Features

### Problem
- Network treats all market conditions the same
- No signal for trending vs. mean-reverting regimes
- Can't gate behavior by volatility/trend state

### Solution
- Add **5 account-invariant regime features**:
  1. `realized_vol_24h_z`: 24h vol z-score (clip Â±5)
  2. `realized_vol_96h_z`: 96h vol z-score (clip Â±5)
  3. `trend_24h`: Sign of 24h lr_slope (-1, 0, +1)
  4. `trend_96h`: Sign of 96h lr_slope (-1, 0, +1)
  5. `is_trending`: Boolean for strong trend + high vol

### Changes Made

**`features.py` - add_regime_features()** (new method, lines ~608-660):
```python
def add_regime_features(self, df):
    # Realized volatility (std of log returns)
    log_ret = np.log(df['close'] / df['close'].shift(1))
    vol_24h = log_ret.rolling(24).std()
    vol_96h = log_ret.rolling(96).std()
    
    # Z-score normalization (clip Â±5)
    vol_24h_z = ((vol_24h - vol_24h.rolling(96).mean()) / 
                 (vol_24h.rolling(96).std() + 1e-9)).clip(-5, 5)
    vol_96h_z = ((vol_96h - vol_96h.rolling(192).mean()) / 
                 (vol_96h.rolling(192).std() + 1e-9)).clip(-5, 5)
    
    # Trend signals
    hlc = (df['high'] + df['low'] + df['close']) / 3.0
    lr_slope_24h = _rolling_lr_slope(hlc, 24)
    lr_slope_96h = _rolling_lr_slope(hlc, 96)
    trend_24h = np.sign(lr_slope_24h)
    trend_96h = np.sign(lr_slope_96h)
    
    # Trending regime detection
    slope_q80 = np.abs(lr_slope_96h).rolling(192).quantile(0.80)
    vol_q60 = vol_24h.rolling(96).quantile(0.60)
    is_trending = ((np.abs(lr_slope_96h) > slope_q80) & 
                   (vol_24h > vol_q60)).astype(float)
    
    # Add to dataframe
    df['realized_vol_24h_z'] = vol_24h_z
    df['realized_vol_96h_z'] = vol_96h_z
    df['trend_24h'] = trend_24h
    df['trend_96h'] = trend_96h
    df['is_trending'] = is_trending
    
    return df
```

**`features.py` - compute_all_features()** (lines ~220):
```python
df['lr_slope'] = self.compute_lr_slope(df)
df = self.add_regime_features(df)  # New
df = df.ffill().bfill().fillna(0)
```

**`features.py` - get_feature_names()** (lines ~545-555):
```python
features = [
    'open', 'high', 'low', 'close',
    'atr', 'rsi',
    'percentile_short', 'percentile_medium', 'percentile_long',
    'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'doy_sin', 'doy_cos',
    'top_fractal_confirmed', 'bottom_fractal_confirmed',
    'lr_slope',
    # Regime meta-features
    'realized_vol_24h_z', 'realized_vol_96h_z',
    'trend_24h', 'trend_96h', 'is_trending'
]
```

### Validation
- Check 5 new features in state vector
- Verify `is_trending` binary (0 or 1)
- Confirm vol z-scores clipped to Â±5

### Expected Impact
- Network can gate strategies by regime (e.g., different actions when trending)
- Better generalization across market conditions
- Potential for regime-specific policies

---

## PATCH 9: Grad-Norm Logging (Observability)

### Problem
- No visibility into gradient magnitudes during training
- Can't diagnose exploding/vanishing gradients
- Hard to tune grad_clip value without feedback

### Solution
- Compute grad norm before clipping
- Store in `agent._last_grad_norm`
- Can log to TensorBoard or print diagnostics

### Changes Made

**`agent.py` - train_step()** (lines ~495-505):
```python
self.optimizer.zero_grad()
loss.backward()

# Compute and store grad norm before clipping
grad_norm = torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.grad_clip)
if not hasattr(self, '_last_grad_norm'):
    self._last_grad_norm = 0.0
self._last_grad_norm = float(grad_norm)

self.optimizer.step()
```

### Validation
- Check `agent._last_grad_norm` exists after training starts
- Typical values: 0.1-10.0 (if much higher, gradients exploding)
- Verify grad_clip=1.0 or 2.0 is appropriate threshold

### Expected Impact
- Better diagnostics for training instability
- Can adjust grad_clip based on actual grad norms
- Early warning for gradient issues

---

## PATCH 10: Cost Budget Assertion & Safety

### Problem
- No check for runaway spread/commission costs
- Frictions can drift silently and ruin training
- Need explicit budget enforcement per episode

### Solution
- **Assert cost budget**: warn if costs exceed 150% of 5% balance
- Already have kill-switch at 100% (trading_locked)
- Add explicit warning for diagnostics

### Changes Made

**`environment.py` - step()** (lines ~400-410):
```python
# Cost budget kill-switch
costs = getattr(self, 'costs_this_ep', 0.0)
budget = 0.05 * self.initial_balance  # 5% of balance

# Assert budget (warning at 150% threshold)
if costs > budget * 1.5:
    import warnings
    warnings.warn(f"Cost budget exceeded: ${costs:.2f} > ${budget:.2f} (150%)")

# Lock trading at 100% budget
if costs > budget and not self.trading_locked:
    self.trading_locked = True
```

### Validation
- Check warning triggers when costs hit $75 (150% of $50 budget on $1000)
- Verify trading_locked=True at $50 (100% budget)
- Monitor episode logs for budget violations

### Expected Impact
- Early detection of friction drift
- Prevents silent failure from excessive costs
- Clear diagnostics when costs too high

---

## Configuration Updates

### SMOKE Mode (â‰¤5 episodes)
**`config.py`** & **`main.py`**:
```python
SMOKE_LEARNING_STARTS = 400  # lowered from 1000
agent.use_noisy = True
agent.noisy_sigma_init = 0.4
agent.epsilon_start = 0.0
agent.epsilon_end = 0.0
agent.update_every = 16  # less frequent updates
agent.grad_steps = 1     # single grad step
```

### Full Runs (20+ episodes)
```python
agent.use_noisy = True
agent.noisy_sigma_init = 0.4
agent.epsilon_start = 0.0
agent.epsilon_end = 0.0
agent.update_every = 4   # standard frequency
agent.grad_steps = 2     # standard steps
patience = 10            # increased from 8
```

---

## Testing Checklist

### PATCH 1 (NoisyNet)
- [ ] Verify `use_noisy=True` in config
- [ ] Check `epsilon_start=0.0`, `epsilon_end=0.0`
- [ ] Confirm `reset_noise()` called in select_action and train_step
- [ ] Observe exploration without epsilon decay

### PATCH 2 (N-step Returns)
- [ ] Replay buffer stores 6-tuples: `(s, a, R_n, s_n, done, n)`
- [ ] Verify `n_step=3` in buffer initialization
- [ ] Check target computation uses `gamma^n` discount
- [ ] Sample transitions and confirm n_step_reward accumulated

### PATCH 3 (Volatility Slippage)
- [ ] Verify slippage varies with ATR
- [ ] Check typical range: 0.5-2.0 pips
- [ ] Log slippage_eff during episodes
- [ ] Confirm higher costs during high ATR

### PATCH 4 (Enhanced MOVE_SL_CLOSER)
- [ ] Log SL movements to verify breakeven trigger at RR=1
- [ ] Check trail activates when in profit
- [ ] Ensure SL only moves closer (never widens)
- [ ] Monitor action 3 usage (should increase vs. baseline)

### PATCH 5 (Holding Cost)
- [ ] Verify reward -= 1e-4 when position held
- [ ] Check flat positions have reward â‰ˆ -1e-4 per step
- [ ] Confirm doesn't dominate PnL signals
- [ ] Monitor average holding duration (should decrease slightly)

### PATCH 6 (Median + Patience 10)
- [ ] Check val_fitness is median of K=7 values
- [ ] Verify EMA updates each validation
- [ ] Confirm early stop at patience=10
- [ ] Compare fitness stability vs. mean baseline

### PATCH 7 (Scaler Persistence)
- [ ] Verify `best_model_scaler.json` created
- [ ] Check JSON contains mu, sig, feature_columns
- [ ] Load checkpoint and confirm scaler applied
- [ ] Test eval with loaded scaler (no distribution shift)

### PATCH 8 (Regime Features)
- [ ] Verify 5 new features in state vector
- [ ] Check `is_trending` is binary (0 or 1)
- [ ] Confirm vol z-scores clipped to Â±5
- [ ] Monitor feature statistics in logs

### PATCH 9 (Grad-Norm Logging)
- [ ] Check `_last_grad_norm` exists after training
- [ ] Typical values: 0.1-10.0
- [ ] Log to TensorBoard: `writer.add_scalar('train/grad_norm', ...)`
- [ ] Adjust grad_clip if norms consistently > 5.0

### PATCH 10 (Cost Budget Assert)
- [ ] Check warning at 150% budget ($75 on $1000)
- [ ] Verify trading_locked at 100% budget ($50)
- [ ] Monitor episode costs in logs
- [ ] Test with artificially high spread to trigger warnings

---

## Expected Outcomes

### Training Speed
- **Faster credit assignment**: 3-step returns â†’ fewer episodes to convergence
- **Learning starts earlier**: SMOKE at 400 steps (vs. 5000 previously)
- **Fewer flat validations**: n-step + NoisyNet reduce early stagnation

### Validation Quality
- **Median fitness**: Resistant to outlier lucky runs
- **EMA smoothing**: Reduces whipsaw in early stopping
- **Patience=10**: Fewer false positives from noise

### Action Quality
- **MOVE_SL_CLOSER useful**: Breakeven + trail â†’ fewer full flips
- **Regime-aware**: Network can gate behavior by market state
- **Less churn**: Holding cost + vol-aware slippage â†’ smarter trading

### Robustness
- **NoisyNet exploration**: Less collapse after good episodes
- **Scaler persistence**: No distribution shift on eval
- **Cost budget**: Catches friction drift early

### Performance Targets (Synthetic Data)
- Validation Sharpe: -0.3 to +0.8 (clearer wins, fewer big negatives)
- Fewer "lucky" high-fitness runs (median filters outliers)
- Action 3 usage: 5-15% of actions (vs. <1% baseline)
- Learning starts: ~360-600 steps in SMOKE (vs. 5000)

---

## Files Modified

1. **`config.py`**: NoisyNet config, SMOKE_LEARNING_STARTS=400
2. **`agent.py`**: N-step buffers, grad-norm logging, train_step updates
3. **`environment.py`**: Vol-aware slippage, enhanced MOVE_SL_CLOSER, holding cost, cost budget assert
4. **`trainer.py`**: Median fitness, patience=10, scaler save/load
5. **`features.py`**: Regime meta-features (5 new features)

---

## Rollback Plan

If issues arise, revert patches individually:
1. **NoisyNet**: Set `use_noisy=False`, restore epsilon values
2. **N-step**: Change `n_step=1` in buffers (reverts to 1-step TD)
3. **Vol slippage**: Use fixed `slippage_pips` (remove ATR component)
4. **MOVE_SL_CLOSER**: Revert to old risk_manager logic
5. **Holding cost**: Remove `reward -= 1e-4` line
6. **Median**: Use `np.mean()` instead of `np.median()`
7. **Scaler**: Skip save/load (recompute per run)
8. **Regime**: Remove 5 features from feature list
9. **Grad-norm**: Comment out `_last_grad_norm` tracking
10. **Cost assert**: Remove warning (keep kill-switch)

---

## Next Steps

1. **Smoke test**: `python main.py --episodes 5`
   - Check learning starts ~360-600 steps
   - Verify NoisyNet active (no epsilon decay logs)
   - Confirm 5 new regime features in state

2. **Full run**: `python main.py --episodes 20`
   - Monitor median fitness stability
   - Check MOVE_SL_CLOSER usage (action 3)
   - Verify early stop at patience=10

3. **Scaler test**: Load best checkpoint, eval on test set
   - Confirm `best_model_scaler.json` exists
   - Check eval metrics match validation

4. **Cost budget test**: Artificially raise spread to trigger warnings
   - Verify warning at 150% budget
   - Confirm trading_locked at 100%

5. **TensorBoard**: Add grad-norm, regime feature histograms
   - `writer.add_scalar('train/grad_norm', agent._last_grad_norm, step)`
   - `writer.add_histogram('state/regime_features', regime_vec, step)`

---

## Maintenance Notes

- **NoisyNet sigma**: Start at 0.4 for synthetic data; may reduce to 0.2-0.3 for live data
- **N-step n**: Can try n=2 or n=4 if variance too high/low
- **Holding cost**: Adjust 1e-4 based on bar frequency (halve for H4, double for M30)
- **Patience**: Can increase to 12-15 for very long runs (>50 episodes)
- **Regime features**: Can add more (RSI divergence, volume z-score) but keep account-invariant

---

**Summary:** All 10 patches successfully applied. System now has:
- âœ… NoisyNet exploration (state-dependent)
- âœ… 3-step returns (faster credit assignment)
- âœ… Vol-aware slippage (dynamic frictions)
- âœ… Enhanced SL management (breakeven + trail)
- âœ… Holding cost (anti-churn)
- âœ… Median fitness (outlier-resistant)
- âœ… Scaler persistence (no distribution shift)
- âœ… Regime features (market-aware)
- âœ… Grad-norm logging (observability)
- âœ… Cost budget assert (safety)

**Ready for testing!** ðŸš€
